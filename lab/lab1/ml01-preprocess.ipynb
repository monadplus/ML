{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 01 - Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "#!pip3 install pandas --user --upgrade --quiet\n",
    "#!pip3 install numpy --user --upgrade --quiet\n",
    "#!pip3 install scipy --user --upgrade --quiet\n",
    "#!pip3 install statsmodels --user --upgrade --quiet\n",
    "#!pip3 install seaborn --user --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra imports\n",
    "from pandas import read_csv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: READING THE FILE CREDSCO.TXT (loan data: credit scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Reading properly a data set is non-trivial because you need to know\n",
    " its data format: decimal separator, column separator, is there a\n",
    " header? how are strings quoted? how (if any) are missing values\n",
    " coded? should character vectors be converted to factors? should\n",
    " white spaces be stripped?, ...)\n",
    "\n",
    " It is a good idea to consult  [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)  and play with useful control parameters.\n",
    " \n",
    " after opening the file `credsco.csv` and inspecting it, we decide the following settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit = read_csv(\"credsco.csv\", header=0, delimiter=',')\n",
    "Credit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Basic questions:\n",
    "\n",
    "* Which is the target variable? where is it? how many different values? is it a classification problem or a regression problem?\n",
    "\n",
    "* *answers:* the target variable is located in column 1 and is called 'Assessment'; it has two possible values (therfore it is a classification problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the other variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect predictive variables 4, 5, 6 and 7 for the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.iloc[0,4:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use the variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.loc[0,'Age':'TypeOfJob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## SECTION 2: BASIC INSPECTION OF THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a basic inspection of the dataset. Have a look at the minimum and maximum values for each variable; find possible errors and abnormal values (outliers); find possible missing values; decide which variables are continuous and which are categorical; if there are mixed types, we have three options: recode continuous to categorical, recode categorical to continuous or leave them as they are. In the latter case, either the method accepts both kinds of information, or it does not, in which case python will convert the categorical ones to continuous using a dummy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `Assessment`,`Housing`,`MaritalStatus`,`Records`,`TypeOfJob` are categorical and need to be treated properly\n",
    "\n",
    " In particular, `Assessment` is the target variable; we need to identify correct values\n",
    "\n",
    " `Capital`, `ChargesOnCapital` and `Income` present abnormally high maximums (99999999)\n",
    "\n",
    " There are also suspicious zeros, in both types of variables, which we identify with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit['Assessment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: DEALING WITH MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need to take a decision on a sensible treatment for the missing values and apply it; it is wise to write down the possible consequences of this decision and the alternatives that could be considered in case the final results are not satisfactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the easiest way is of course to eliminate the involved rows or\n",
    "columns; this can be done partially. For example, we could decide to\n",
    "eliminate the variables with the highest proportion of missing values.\n",
    "\n",
    "Deleting instances and/or variables containing missing values results\n",
    "in loss of relevant data and is also frustrating because of the effort\n",
    "in collecting the sacrificed information.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "CAREFUL! python does not know magically which entries are missing values: they have to be explicitly declared as NA's\n",
    "</div>\n",
    "\n",
    "therefore this code is not useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_complete = Credit.dropna()\n",
    "Credit_complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the previous code does nothing! (but it seems it does)\n",
    "\n",
    " In the present case we have decided to perform a step-by-step treatment, separate for the categorical and continuous information\n",
    "\n",
    " We first decide to remove those rows with with missing values in the categorical variables (there are few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Credit.Assessment==0).value_counts()\n",
    "(Credit.Housing==0).value_counts()\n",
    "(Credit.MaritalStatus==0).value_counts()\n",
    "(Credit.TypeOfJob==0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit=  Credit[(Credit.Assessment!=0) & (Credit.Housing!=0)\n",
    "                &(Credit.MaritalStatus!=0)&(Credit.TypeOfJob!=0)]\n",
    "Credit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Process rows with missing values in the continuous variables (code 99999999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " look at that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.Income.hist(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.Income[Credit.Income!=99999999].hist(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.Income[(Credit.Income!=99999999)&\n",
    "              (Credit.Income!=0)].hist(bins=15,figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " these are then clearly incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Credit.Income==99999999).value_counts()\n",
    "(Credit.Income==0).value_counts()\n",
    "(Credit.Capital==99999999).value_counts()\n",
    "(Credit.ChargesOnCapital==99999999).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Credit.Income==99999999).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " what do we do with this one? let's assume it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Credit.YearsInJob==0).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Continuous variables have too many missing values, we can not eliminate them just like that: we must devise a treatment for these missing values\n",
    "\n",
    " first we mark them to 'NA', including those from no 'Income'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.Income[(Credit.Income == 99999999) | (Credit.Income == 0)] = np.nan\n",
    "Credit.Capital[Credit.Capital == 99999999]  = np.nan\n",
    "Credit.ChargesOnCapital[Credit.ChargesOnCapital == 99999999]  = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.Income.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The word 'imputation' refers to assigning a value to every missing value. Here we perform imputation by a method known as 1NN: for every individual with a missing 'Income', we look for the most similar individual (according to the remaining variables) and then copy its 'Income' value.\n",
    " \n",
    " As we can not have missing values in any column for computing the 1KNN we will make a classifier dropping the columns with missing values and training a classifier for each column\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_of_credit = Credit.drop(columns=['Income', 'Capital', 'ChargesOnCapital'])\n",
    "subset_of_credit.shape\n",
    "\n",
    "rows_not_missing = Credit.Income.notna() & Credit.Capital.notna() & Credit.ChargesOnCapital.notna()\n",
    "\n",
    "credit_without_missings = subset_of_credit[rows_not_missing]\n",
    "credit_without_missings.shape\n",
    "\n",
    "credit_where_income_has_missing_values = subset_of_credit[Credit.Income.isna()]\n",
    "credit_where_income_has_missing_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Neither of credit_without_missings, credit_where_income_has_missing_values can contain NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(credit_without_missings, Credit.Income[rows_not_missing])\n",
    "knn_inc = knn.predict(credit_where_income_has_missing_values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Imputation of 'Capital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_missing_values = subset_of_credit[Credit.Capital.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(credit_without_missings, Credit.Capital[rows_not_missing])\n",
    "knn_cap = knn.predict(capital_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation of 'ChargesOnCapital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charges_missing_values = subset_of_credit[Credit.ChargesOnCapital.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(credit_without_missings, Credit.ChargesOnCapital[rows_not_missing])\n",
    "knn_cop = knn.predict(charges_missing_values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Credit.Income[Credit.Income.isna()] = knn_inc\n",
    "Credit.Capital[Credit.Capital.isna()] = knn_cap\n",
    "Credit.ChargesOnCapital[Credit.ChargesOnCapital.isna()] = knn_cop\n",
    "Credit.ChargesOnCapital[Credit.Capital==0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are computanionally cheaper methods for missing value imputations such as replacing with mean, median or mode. All these can be computed using pandas `replace` and `fillna` functions, you can find more info [here](https://pandas.pydata.org/pandas-docs/stable/missing_data.html)\n",
    "\n",
    "The Scikit-learn library has also a method for the most simple missing value imputation [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect again the result, especially the new statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Credit.shape\n",
    "Credit.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: FINDING OUTLIERS\n",
    "\n",
    "In real data you are usually going to find outliers. It can be because the data is just like that or because there are wrong values stored. \n",
    "\n",
    "It is important to identify them so you can remove them, impute them, or just acknowledge their existence and take into account in your analysis. \n",
    "\n",
    "Some machine learning models are very sensitive to outliers. \n",
    "\n",
    "There are a lot of ways to define an outlier. Here we are going to talk about a basic and a complex one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))\n",
    "Credit.boxplot(column='MarketPrice',ax=axes[0]);\n",
    "Credit.hist(column='MarketPrice', ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR \n",
    "\n",
    "One easy criteria to decide which values are outliers is to use the distance between quartiles. Specifically, you define the outliers as: \n",
    "* The values smaller than Q1 - 1.5*IQR\n",
    "* The values bigger than Q3 + 1.5*IQR\n",
    "\n",
    "This is the criteria used in the boxplots to marc the outliers. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/1024px-Boxplot_vs_PDF.svg.png\" alt=\"nice_image_from_wikipedia\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used this criteria to remove the outliers of our variable we would get the next distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = Credit['MarketPrice'].quantile(0.25)\n",
    "Q3 = Credit['MarketPrice'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "Q1, Q3, IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_outliers = Credit['MarketPrice'] < (Q1 - 1.5 * IQR)\n",
    "big_outliers = Credit['MarketPrice'] > (Q3 + 1.5 * IQR)\n",
    "\n",
    "sum(small_outliers), sum(big_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit['MarketPrice'][small_outliers | big_outliers].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))\n",
    "Credit[~(small_outliers | big_outliers)].boxplot(column='MarketPrice',ax=axes[0]);\n",
    "Credit[~(small_outliers | big_outliers)].hist(column='MarketPrice', ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model based\n",
    "Sometimes the classic approaches will not be enough to detect outliers. The data can take many forms, like images, text or just pairs of variables that would not make sense to take independenly for any reason. \n",
    "\n",
    "In these cases we will want to use more complex approaches. Here we are going to talk about [Local Outlier Factor](https://dl.acm.org/doi/abs/10.1145/342009.335388), but there are a lot more. \n",
    "\n",
    "Local outlier factor uses the local density of a point to decide whether it is an otlier or not. The local density is calculated by using its k nearest neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "If you use a model-based approach you will need to tune its hyperparameters. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_outlier_factor = LocalOutlierFactor(n_neighbors=20)\n",
    "result = local_outlier_factor.fit_predict(Credit['MarketPrice'].values.reshape(-1, 1))\n",
    "\n",
    "outliers = result == -1 \n",
    "no_outliers = result == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))\n",
    "Credit[no_outliers].boxplot(column='MarketPrice',ax=axes[0]);\n",
    "Credit[no_outliers].hist(column='MarketPrice', ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## SECTION 5: TREATMENT OF MIXED DATA TYPES\n",
    "\n",
    "\n",
    " In this case we have decided to keep the original type and leave the decision for later, depending on the specific analysis\n",
    "\n",
    " we explicitly declare categorical variables as such "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a categorical datatype in pandas, but for most things this will do\n",
    "Credit.Assessment = Credit.Assessment.astype('object')\n",
    "Credit.Housing = Credit.Housing.astype('object')\n",
    "Credit.MaritalStatus = Credit.MaritalStatus.astype('object')\n",
    "Credit.Records = Credit.Records.astype('object')\n",
    "Credit.TypeOfJob = Credit.TypeOfJob.astype('object')\n",
    "\n",
    "Credit.Assessment.unique()\n",
    "Credit.Housing .unique()\n",
    "Credit.MaritalStatus.unique()\n",
    "Credit.Records.unique()\n",
    "Credit.TypeOfJob.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not very nice, right? let's recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.Assessment.replace([1, 2], \n",
    "                          [\"positive\",\"negative\"], \n",
    "                          inplace=True)\n",
    "Credit.Housing.replace([1,2,3,4,5,6], \n",
    "                       [\"rent\",\"owner\",\"private\",\"ignore\",\"parents\",\"other\"], \n",
    "                       inplace=True)\n",
    "Credit.MaritalStatus.replace([1,2,3,4,5], \n",
    "                             [\"single\",\"married\",\"widower\",\"split\",\"divorced\"], \n",
    "                             inplace=True)\n",
    "Credit.Records.replace([1, 2], \n",
    "                       [\"no\",\"yes\"], inplace=True)\n",
    "Credit.TypeOfJob.replace([1,2,3,4], \n",
    "                         [\"indefinite\",\"temporal\",\"self-employed\",\"other\"], \n",
    "                         inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "If you want to treat a categorical variable as numerical you should never replace the categories by numbers. \n",
    "If you do so you are adding an order relationship that did not exist previously in the data, adding noise to your analysis. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## SECTION 6: DERIVATION OF NEW VARIABLES: FEATURE EXTRACTION\n",
    "\n",
    " We decide whether it can be sensible to derive new variables; we extract two new continuous and one new categorical variable (for the sake of illustration):\n",
    "\n",
    " Financing ratio (continuous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit['FinancingRatio'] = 100*Credit.AmountRequested/Credit.MarketPrice\n",
    "Credit.FinancingRatio.hist(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Saving capacity (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit['SavingCapacity'] = (Credit.Income- Credit.Expenses-(Credit.ChargesOnCapital/100))\\\n",
    "                            /(Credit.AmountRequested/Credit.Deadline)\n",
    "Credit.SavingCapacity.hist(bins=16,figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Amount Requested greater than the median by people younger than 1.25 times the mean (categorical):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit['Dubious'] = ['No']*Credit.shape[0]\n",
    "Credit.Dubious[(Credit.AmountRequested > Credit.AmountRequested.median(skipna=True)) &\n",
    "               (Credit.Age < 1.25*Credit.Age.mean(skipna=True))] = \"Yes\"\n",
    "pd.crosstab(Credit.Dubious, Credit.Assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## SECTION 7: WHAT WE HAVE DONE SO FAR\n",
    "\n",
    "\n",
    " Create a new dataframe that gathers everything and inspect it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new =Credit.copy()\n",
    "                   \n",
    "Credit_new.describe(include='all')\n",
    "Credit_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## SECTION 8: GAUSSIANITY AND TRANSFORMATIONS\n",
    "\n",
    "\n",
    " Perform a graphical summary of some of the variables (both categorical and continuous), using the boxplot() and hist() procedures\n",
    "\n",
    " For continuous data:\n",
    " histograms and boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.Income.hist(bins=20,figsize=(8,8), color='green');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.Capital.hist(bins=20,figsize=(8,8), color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.use_inf_as_na = True\n",
    "Credit_new.Capital.apply(np.log10).hist(bins=20,figsize=(8,8), color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.boxplot(column='Deadline',figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Credit_new.boxplot(column='Age',figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.loc[:,\"Expenses\":\"SavingCapacity\"].boxplot(figsize=(14,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.loc[:,\"Expenses\":\"SavingCapacity\"].boxplot(figsize=(14,8),showfliers=False); # much better, but would be nicer one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the previous plots suggest to take logs on some variables: Capital and ChargesOnCapital (we'll do it later)\n",
    "\n",
    " For categorical data:\n",
    " Frequency tables, Contingency tables, Bar charts, Pie charts\n",
    "\n",
    " should we treat Age as categorical? probably not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform numerical variables into categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.Age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.Age.min()\n",
    "Credit_new.Age.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.interval_range(start=30, end=90,freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(Credit_new.Age,\n",
    "       bins=pd.interval_range(start=30, end=90,freq=10))  \n",
    "# WARNING! we are generating NAs               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age_cut = pd.cut(Credit_new.Age,\n",
    "                 bins=pd.interval_range(start=15, end=75,freq=10)) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new['Age_cat'] = Age_cut.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.Age_cat.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.Age_cat.value_counts().sort_index().plot.bar(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.Age_cat.value_counts().sort_index().plot.pie(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " incidentally, this is how we could generate another new variable based on Age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new['Age2_cat'] = Credit_new.Age.apply(lambda x : \n",
    "                                              'under55' if x < 55 else 'over55')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeOfJob_Age= pd.crosstab(Credit_new.TypeOfJob, Credit_new.Age_cat)\n",
    "TypeOfJob_Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(TypeOfJob_Age, annot=True, fmt=\"d\", cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeOfJob_Age.sum(axis=0) # row sums\n",
    "TypeOfJob_Age.sum(axis=1) # column sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(Credit_new.TypeOfJob, \n",
    "            Credit_new.Age_cat,\n",
    "            normalize=True,\n",
    "            margins=True) # relative frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(Credit_new.TypeOfJob, \n",
    "            Credit_new.Age_cat,\n",
    "            normalize=True,margins=True).round(decimals=3)\n",
    "            # idem, rounded to 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(Credit_new.TypeOfJob,\n",
    "             Credit_new.Age_cat,\n",
    "             normalize=True,\n",
    "             margins=True)*100).round(decimals=3) \n",
    "             # total percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(Credit_new.TypeOfJob,\n",
    "            Credit_new.Age_cat,\n",
    "            normalize=\"index\").round(decimals=3) \n",
    "            # table of relative frequencies (column-wise)\n",
    "pd.crosstab(Credit_new.TypeOfJob, \n",
    "            Credit_new.Age_cat,\n",
    "            normalize=\"columns\").round(decimals=3) \n",
    "            # table of relative frequencies (row-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stacked bar chart\n",
    "TypeOfJob_Age.T.plot.bar(stacked=True, figsize=(8,8));  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(Credit_new.TypeOfJob, \n",
    "            Credit_new.Age_cat,\n",
    "            normalize=\"columns\").T.plot.bar(stacked=True, figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped bar chart\n",
    "pd.crosstab(Credit_new.TypeOfJob, \n",
    "            Credit_new.Age_cat,\n",
    "            normalize=\"columns\").T.plot.bar(figsize=(8,8));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  One hot encoding (dummy encoding)\n",
    "\n",
    "Sometimes you will want to treat some (of all) your categorical variables as numerical. This can happen if, for example you want to train a model that does not accept categorical variables. \n",
    "There are different methods to codify the categories. One of the most simple and comonly used is one hot encoding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit['Records'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(Credit['Records'], prefix='Records').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.plot.scatter(y='Capital',\n",
    "                        x='AmountRequested', \n",
    "                        figsize=(8,8),\n",
    "                        title='Amount req. vs. Capital');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new['Capital_log10'] = (Credit_new.Capital+1).apply(np.log10)\n",
    "Credit_new['AmountRequested_log10'] = (Credit_new.AmountRequested).apply(np.log10)\n",
    "Credit_new.plot.scatter(y='Capital_log10',\n",
    "                        x='AmountRequested_log10', \n",
    "                        figsize=(8,8),\n",
    "                        title='log Amount req. vs. log Capital (better)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " adding a center (dashed) and a regression line (red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GLM.from_formula('Capital_log10 ~ AmountRequested_log10', Credit_new)\n",
    "result = model.fit()\n",
    "\n",
    "Credit_new.plot.scatter(y='Capital_log10',\n",
    "                        x='AmountRequested_log10', \n",
    "                        figsize=(8,8))\n",
    "plt.title('Amount req. vs. Capital (better)');\n",
    "plt.plot([Credit_new.AmountRequested_log10.mean()]*2,\n",
    "         [Credit_new.Capital_log10.min(),\n",
    "          Credit_new.Capital_log10.max()],\n",
    "         'k:');\n",
    "plt.plot([Credit_new.AmountRequested_log10.min(),\n",
    "          Credit_new.AmountRequested_log10.max()],\n",
    "         [Credit_new.Capital_log10.mean()]*2,\n",
    "         'k:');\n",
    "plt.plot(np.linspace(Credit_new.AmountRequested_log10.min(), \n",
    "                     Credit_new.AmountRequested_log10.max(),\n",
    "                     num=30),\n",
    "         result.params.Intercept+\n",
    "         result.params.AmountRequested_log10*\n",
    "         np.linspace(Credit_new.AmountRequested_log10.min(),                                                             \n",
    "                     Credit_new.AmountRequested_log10.max(),num=30),'r');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (note that log10(x+1)=0 for x=0, so our transformation keeps the zeros)\n",
    "     \n",
    " On the other hand, these same zeros spoil the regression: perhaps it would be more sensible to do the regression without them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.boxplot(column='AmountRequested',\n",
    "                   by='TypeOfJob',\n",
    "                   figsize=(12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.boxplot(column='Age',\n",
    "                   by='Assessment',\n",
    "                   figsize=(12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(Credit_new.TypeOfJob, \n",
    "            Credit_new.Assessment,\n",
    "            normalize=\"columns\").T.plot.bar(stacked=True, \n",
    "                                            figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(Credit_new.loc[:,['AmountRequested','Capital','Age']], \n",
    "               alpha=0.2, figsize=(12, 12), \n",
    "               diagonal='kde', marker='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussianity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=  Credit_new.Expenses.std()\n",
    "mu=  Credit_new.Expenses.mean()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "count, bins, ignored = plt.hist(Credit_new.Expenses, 15, density=True)\n",
    "dbins = np.linspace(bins[0], bins[-1])\n",
    "plt.title(\"Expenses\")\n",
    "ax.set_xlabel(\"Expenses\")\n",
    "plt.plot(dbins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "                np.exp( - (dbins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "for i in range(8):\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    Credit_new[Credit.columns[i+8]].plot.kde()\n",
    "    plt.title(Credit.columns[i+8])\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.loc[:,'Expenses':'SavingCapacity'].plot.kde(subplots=True,\n",
    "                                                       layout=(4,3),\n",
    "                                                       sharex=False, \n",
    "                                                       figsize=(12,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " do any of the continuous variables \"look\" Gaussian? \n",
    " features to look for in comparing to a Gaussian: outliers, asymmetries, long tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION:9 NORMALIZATION\n",
    "\n",
    "If you try to train a model with varaibles of ranges too far away it will be a disaster most of the times. Becasue the model might only \"see\" the bigger variable. \n",
    "\n",
    "To avoid this issue you usualy normalize or standarize your data. This way you force all your variables to have the same range. \n",
    "There are models that are very sensitive to this and might even fail to converge if you don't normalize your data. \n",
    "\n",
    "The most comon transformations for normalizing the data are: \n",
    "* Standarization: $\\frac{X - \\mu}{\\sigma}$ will trasnform your data so it has mean 0 and std 1.\n",
    "* Min-max scaling: $\\frac{X - X_{min}}{X_{max} - X_{min}}$ will send your data to the range [0,1]\n",
    "* Boxcox transformation: $\\frac{X^\\lambda - 1}{ \\lambda}$ if $\\lambda \\neq 0$ or $ln(X)$ if $\\lambda = 0$ transforms the data to try to fit a normal distribution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new[['Age', 'MarketPrice']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler =  preprocessing.MinMaxScaler()\n",
    "Credit_new[['Age_min_max', 'MarketPrice_min_max']] = min_max_scaler.fit_transform(Credit_new[['Age', 'MarketPrice']])\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "Credit_new[['Age_std','MarketPrice_std']]= preprocessing.scale(Credit_new[['Age', 'MarketPrice']])\n",
    "\n",
    "x, _= boxcox(Credit_new['Age'])\n",
    "Credit_new['Age_bx'] = x\n",
    "\n",
    "x, _= boxcox(Credit_new['MarketPrice'])\n",
    "Credit_new['MarketPrice_bx'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new[['Age_min_max', 'MarketPrice_min_max','Age_std', 'MarketPrice_std', 'Age_bx','MarketPrice_bx']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new[['Age','Age_min_max', 'Age_std', 'Age_bx']].hist(figsize=(14,4), layout=(1,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new[['MarketPrice','MarketPrice_min_max', 'MarketPrice_std', 'MarketPrice_bx']].hist(figsize=(14,4), layout=(1,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra plot boxcox transformation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "Credit_new.Capital.plot.hist(title='Look at that ...')\n",
    "\n",
    "# the boxcox function transforms the data using \n",
    "# the power transformation (x**lambda -1)/ lambda\n",
    "# the function takes care of finding the optimal lambda\n",
    "x, _= boxcox(Credit_new.Capital+1)\n",
    "\n",
    "Credit_new['Capital_BC'] = x\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "Credit_new.Capital_BC.plot.hist(title='Look at that now ...');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 10: ENDING THE PREPROCESSING\n",
    "\n",
    "  \n",
    " Shuffle the data (to avoid possible ordering biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(144)\n",
    "Credit_new = Credit_new.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " Save the preprocessed data into a file for future use\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Pandas allows to save the data in a lot of different formats as you can see [here](http://pandas-docs.github.io/pandas-docs-travis/io.html) among others CSV, pickle, HDF5, JSON, Excel as well as other data storages like SQL databases, Google Big Query, parquet or feather.\n",
    "\n",
    "The simplest way is to save the data as a csv with `to_csv` or as a pickle file (native python store format) with `to_pickle`, this last one allows also to compress the data.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_new.to_pickle('Credsco-processed.pkl.bz2',compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_load =pd.read_pickle('Credsco-processed.pkl.bz2',compression='bz2')\n",
    "Credit_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
